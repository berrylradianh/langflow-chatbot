{"id":"f41a3612-8bc3-4580-ae8b-7c64c4b19ac2","data":{"nodes":[{"id":"ChatInput-MyCx4","type":"genericNode","position":{"x":-56.8379945265861,"y":162.5238098399192},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"name":"files","value":"","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":" What are your hours of operation?","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"User","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"ChatInput-MyCx4"},"selected":false,"width":384,"height":300,"positionAbsolute":{"x":-56.8379945265861,"y":162.5238098399192},"dragging":false},{"id":"GroqModel-3OwWV","type":"genericNode","position":{"x":1673.0332078519032,"y":-272.49413193700485},"data":{"type":"GroqModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import requests\nfrom typing import List\nfrom langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = \"Groq\"\n    description: str = \"Generate text using Groq.\"\n    icon = \"Groq\"\n    name = \"GroqModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        SecretStrInput(\n            name=\"groq_api_key\",\n            display_name=\"Groq API Key\",\n            info=\"API key for the Groq API.\",\n        ),\n        MessageTextInput(\n            name=\"groq_api_base\",\n            display_name=\"Groq API Base\",\n            info=\"Base URL path for API requests, leave blank if not using a proxy or service emulator.\",\n            advanced=True,\n            value=\"https://api.groq.com\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            value=0.1,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[],\n            refresh_button=True,\n        ),\n    ]\n\n    def get_models(self) -> List[str]:\n        api_key = self.groq_api_key\n        base_url = self.groq_api_base or \"https://api.groq.com\"\n        url = f\"{base_url}/openai/v1/models\"\n\n        headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n\n        try:\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model[\"id\"] for model in model_list.get(\"data\", [])]\n        except requests.RequestException as e:\n            self.status = f\"Error fetching models: {str(e)}\"\n            return []\n\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name == \"groq_api_key\" or field_name == \"groq_api_base\" or field_name == \"model_name\":\n            models = self.get_models()\n            build_config[\"model_name\"][\"options\"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        output = ChatGroq(  # type: ignore\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key),\n            streaming=stream,\n        )\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"groq_api_base":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"groq_api_base","value":"https://api.groq.com","display_name":"Groq API Base","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Base URL path for API requests, leave blank if not using a proxy or service emulator.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"groq_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"name":"groq_api_key","value":"","display_name":"Groq API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"API key for the Groq API.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Output Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model_name":{"trace_as_metadata":true,"options":["distil-whisper-large-v3-en","llava-v1.5-7b-4096-preview","llama3-8b-8192","llama-3.2-90b-text-preview","llama-3.1-8b-instant","llama-3.2-3b-preview","gemma2-9b-it","llama-3.2-11b-vision-preview","whisper-large-v3","mixtral-8x7b-32768","llama-3.2-1b-preview","llama-3.1-70b-versatile","llama3-70b-8192","llama-guard-3-8b","llama-3.2-11b-text-preview","llama3-groq-8b-8192-tool-use-preview","gemma-7b-it","llama3-groq-70b-8192-tool-use-preview"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"llama3-8b-8192","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n","value":"","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Groq.","icon":"Groq","base_classes":["LanguageModel","Message"],"display_name":"Groq","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","groq_api_key","groq_api_base","max_tokens","temperature","n","model_name"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"GroqModel-3OwWV"},"selected":false,"width":384,"height":617,"dragging":false,"positionAbsolute":{"x":1673.0332078519032,"y":-272.49413193700485}},{"id":"ChatOutput-TYYi9","type":"genericNode","position":{"x":2183.4003848137727,"y":-157.24721001818568},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"ChatOutput-TYYi9"},"selected":false,"width":384,"height":300,"positionAbsolute":{"x":2183.4003848137727,"y":-157.24721001818568},"dragging":false},{"id":"Memory-wsoOV","type":"genericNode","position":{"x":-23.451562339049417,"y":-387.66142387403306},"data":{"type":"Memory","node":{"template":{"_type":"Component","memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"memory","value":"","display_name":"External Memory","advanced":false,"input_types":["BaseChatMessageHistory"],"dynamic":false,"info":"Retrieve messages from an external memory. If empty, it will use the Langflow tables.","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain.memory import ConversationBufferMemory\n\nfrom langflow.custom import Component\nfrom langflow.field_typing import BaseChatMemory\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import LCBuiltinChatMemory, get_messages\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Filter by sender type.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Filter by sender name.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            # langchain memories are supposed to return messages in ascending order\n            if order == \"DESC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        if self.memory:\n            chat_memory = self.memory\n        else:\n            chat_memory = LCBuiltinChatMemory(flow_id=self.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"n_messages":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"n_messages","value":100,"display_name":"Number of Messages","advanced":true,"dynamic":false,"info":"Number of messages to retrieve.","title_case":false,"type":"int","_input_type":"IntInput"},"order":{"trace_as_metadata":true,"options":["Ascending","Descending"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"order","value":"Ascending","display_name":"Order","advanced":true,"dynamic":false,"info":"Order of the messages.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User","Machine and User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine and User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Filter by sender type.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Filter by sender name.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{sender_name}: {text}","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Retrieves stored chat messages from Langflow tables or an external memory.","icon":"message-square-more","base_classes":["BaseChatMemory","Data","Message"],"display_name":"Chat Memory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"messages","display_name":"Messages (Data)","method":"retrieve_messages","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"messages_text","display_name":"Messages (Text)","method":"retrieve_messages_as_text","value":"__UNDEFINED__","cache":true},{"types":["BaseChatMemory"],"selected":"BaseChatMemory","name":"lc_memory","display_name":"Memory","method":"build_lc_memory","value":"__UNDEFINED__","cache":true}],"field_order":["memory","sender","sender_name","n_messages","session_id","order","template"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"Memory-wsoOV"},"selected":false,"width":384,"height":378,"positionAbsolute":{"x":-23.451562339049417,"y":-387.66142387403306},"dragging":false},{"id":"Prompt-MWjss","type":"genericNode","position":{"x":1064.4472752356246,"y":-318.8631257594816},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Youre acting a role as sustomer service at Marvit\n\n{vector_db_result}\n\n{chat_history}\n\nuser_question: {user_question}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"chat_history":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"chat_history","display_name":"chat_history","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"user_question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user_question","display_name":"user_question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"vector_db_result":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"vector_db_result","display_name":"vector_db_result","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["vector_db_result","chat_history","user_question"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"id":"Prompt-MWjss"},"selected":false,"width":384,"height":585,"dragging":false,"positionAbsolute":{"x":1064.4472752356246,"y":-318.8631257594816}},{"id":"VectaraRAG-dnNiH","type":"genericNode","position":{"x":628.1577352186433,"y":303.7523159769521},"data":{"type":"VectaraRAG","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, StrInput, SecretStrInput, Output\nfrom langflow.schema.message import Message\n\n\nclass VectaraRagComponent(Component):\n    display_name = \"Vectara RAG\"\n    description = \"Vectara's full end to end RAG\"\n    documentation = \"https://docs.vectara.com/docs\"\n    icon = \"Vectara\"\n    name = \"VectaraRAG\"\n    SUMMARIZER_PROMPTS = [\n        \"vectara-summary-ext-24-05-sml\",\n        \"vectara-summary-ext-24-05-med-omni\",\n        \"vectara-summary-ext-24-05-large\",\n        \"vectara-summary-ext-24-05-med\",\n        \"vectara-summary-ext-v1.3.0\",\n    ]\n\n    RERANKER_TYPES = [\"mmr\", \"rerank_multilingual_v1\", \"none\"]\n\n    RESPONSE_LANGUAGES = [\n        \"auto\",\n        \"eng\",\n        \"spa\",\n        \"fra\",\n        \"zho\",\n        \"deu\",\n        \"hin\",\n        \"ara\",\n        \"por\",\n        \"ita\",\n        \"jpn\",\n        \"kor\",\n        \"rus\",\n        \"tur\",\n        \"fas\",\n        \"vie\",\n        \"tha\",\n        \"heb\",\n        \"nld\",\n        \"ind\",\n        \"pol\",\n        \"ukr\",\n        \"ron\",\n        \"swe\",\n        \"ces\",\n        \"ell\",\n        \"ben\",\n        \"msa\",\n        \"urd\",\n    ]\n\n    field_order = [\"vectara_customer_id\", \"vectara_corpus_id\", \"vectara_api_key\", \"search_query\", \"reranker\"]\n\n    inputs = [\n        StrInput(name=\"vectara_customer_id\", display_name=\"Vectara Customer ID\", required=True),\n        StrInput(name=\"vectara_corpus_id\", display_name=\"Vectara Corpus ID\", required=True),\n        SecretStrInput(name=\"vectara_api_key\", display_name=\"Vectara API Key\", required=True),\n        MessageTextInput(name=\"search_query\", display_name=\"Search Query\", info=\"The query to receive an answer on.\"),\n        FloatInput(\n            name=\"lexical_interpolation\",\n            display_name=\"Hybrid Search Factor\",\n            range_spec=RangeSpec(min=0.005, max=0.1, step=0.005),\n            value=0.005,\n            advanced=True,\n            info=\"How much to weigh lexical scores compared to the embedding score. 0 means lexical search is not used at all, and 1 means only lexical search is used.\",\n        ),\n        MessageTextInput(\n            name=\"filter\",\n            display_name=\"Metadata Filters\",\n            value=\"\",\n            advanced=True,\n            info=\"The filter string to narrow the search to according to metadata attributes.\",\n        ),\n        DropdownInput(\n            name=\"reranker\",\n            display_name=\"Reranker Type\",\n            options=RERANKER_TYPES,\n            value=RERANKER_TYPES[0],\n            info=\"How to rerank the retrieved search results.\",\n        ),\n        IntInput(\n            name=\"reranker_k\",\n            display_name=\"Number of Results to Rerank\",\n            value=50,\n            range_spec=RangeSpec(min=1, max=100, step=1),\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"diversity_bias\",\n            display_name=\"Diversity Bias\",\n            value=0.2,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n            info=\"Ranges from 0 to 1, with higher values indicating greater diversity (only applies to MMR reranker).\",\n        ),\n        IntInput(\n            name=\"max_results\",\n            display_name=\"Max Results to Summarize\",\n            value=7,\n            range_spec=RangeSpec(min=1, max=100, step=1),\n            advanced=True,\n            info=\"The maximum number of search results to be available to the prompt.\",\n        ),\n        DropdownInput(\n            name=\"response_lang\",\n            display_name=\"Response Language\",\n            options=RESPONSE_LANGUAGES,\n            value=\"eng\",\n            advanced=True,\n            info=\"Use the ISO 639-1 or 639-3 language code or auto to automatically detect the language.\",\n        ),\n        DropdownInput(\n            name=\"prompt\",\n            display_name=\"Prompt Name\",\n            options=SUMMARIZER_PROMPTS,\n            value=SUMMARIZER_PROMPTS[0],\n            advanced=True,\n            info=\"Only vectara-summary-ext-24-05-sml is for Growth customers; all other prompts are for Scale customers only.\",\n        ),\n    ]\n\n    outputs = [\n        Output(name=\"answer\", display_name=\"Answer\", method=\"generate_response\"),\n    ]\n\n    def generate_response(\n        self,\n    ) -> Message:\n        text_output = \"\"\n\n        try:\n            from langchain_community.vectorstores import Vectara\n            from langchain_community.vectorstores.vectara import RerankConfig, SummaryConfig, VectaraQueryConfig\n        except ImportError:\n            raise ImportError(\"Could not import Vectara. Please install it with `pip install langchain-community`.\")\n\n        vectara = Vectara(self.vectara_customer_id, self.vectara_corpus_id, self.vectara_api_key)\n        rerank_config = RerankConfig(self.reranker, self.reranker_k, self.diversity_bias)\n        summary_config = SummaryConfig(\n            is_enabled=True, max_results=self.max_results, response_lang=self.response_lang, prompt_name=self.prompt\n        )\n        config = VectaraQueryConfig(\n            lambda_val=self.lexical_interpolation,\n            filter=self.filter,\n            summary_config=summary_config,\n            rerank_config=rerank_config,\n        )\n        rag = vectara.as_rag(config)\n        response = rag.invoke(self.search_query, config={\"callbacks\": self.get_langchain_callbacks()})\n\n        text_output = response[\"answer\"]\n\n        return Message(text=text_output)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"diversity_bias":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":1,"step":0.01},"list":false,"required":false,"placeholder":"","show":true,"name":"diversity_bias","value":0.2,"display_name":"Diversity Bias","advanced":true,"dynamic":false,"info":"Ranges from 0 to 1, with higher values indicating greater diversity (only applies to MMR reranker).","title_case":false,"type":"float","_input_type":"FloatInput"},"filter":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"filter","value":"","display_name":"Metadata Filters","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The filter string to narrow the search to according to metadata attributes.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"lexical_interpolation":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0.005,"max":0.1,"step":0.005},"list":false,"required":false,"placeholder":"","show":true,"name":"lexical_interpolation","value":0.005,"display_name":"Hybrid Search Factor","advanced":true,"dynamic":false,"info":"How much to weigh lexical scores compared to the embedding score. 0 means lexical search is not used at all, and 1 means only lexical search is used.","title_case":false,"type":"float","_input_type":"FloatInput"},"max_results":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":1,"max":100,"step":1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_results","value":7,"display_name":"Max Results to Summarize","advanced":true,"dynamic":false,"info":"The maximum number of search results to be available to the prompt.","title_case":false,"type":"int","_input_type":"IntInput"},"prompt":{"trace_as_metadata":true,"options":["vectara-summary-ext-24-05-sml","vectara-summary-ext-24-05-med-omni","vectara-summary-ext-24-05-large","vectara-summary-ext-24-05-med","vectara-summary-ext-v1.3.0"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"prompt","value":"vectara-summary-ext-24-05-sml","display_name":"Prompt Name","advanced":true,"dynamic":false,"info":"Only vectara-summary-ext-24-05-sml is for Growth customers; all other prompts are for Scale customers only.","title_case":false,"type":"str","_input_type":"DropdownInput"},"reranker":{"trace_as_metadata":true,"options":["mmr","rerank_multilingual_v1","none"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"reranker","value":"none","display_name":"Reranker Type","advanced":false,"dynamic":false,"info":"How to rerank the retrieved search results.","title_case":false,"type":"str","_input_type":"DropdownInput"},"reranker_k":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":1,"max":100,"step":1},"list":false,"required":false,"placeholder":"","show":true,"name":"reranker_k","value":50,"display_name":"Number of Results to Rerank","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"int","_input_type":"IntInput"},"response_lang":{"trace_as_metadata":true,"options":["auto","eng","spa","fra","zho","deu","hin","ara","por","ita","jpn","kor","rus","tur","fas","vie","tha","heb","nld","ind","pol","ukr","ron","swe","ces","ell","ben","msa","urd"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"response_lang","value":"eng","display_name":"Response Language","advanced":true,"dynamic":false,"info":"Use the ISO 639-1 or 639-3 language code or auto to automatically detect the language.","title_case":false,"type":"str","_input_type":"DropdownInput"},"search_query":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"search_query","value":"","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The query to receive an answer on.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"vectara_api_key":{"load_from_db":false,"required":true,"placeholder":"","show":true,"name":"vectara_api_key","value":"","display_name":"Vectara API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"vectara_corpus_id":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"name":"vectara_corpus_id","value":"3","display_name":"Vectara Corpus ID","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"vectara_customer_id":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"name":"vectara_customer_id","value":"2156734214","display_name":"Vectara Customer ID","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"}},"description":"Vectara's full end to end RAG","icon":"Vectara","base_classes":["Message"],"display_name":"Vectara RAG","documentation":"https://docs.vectara.com/docs","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"answer","display_name":"Answer","method":"generate_response","value":"__UNDEFINED__","cache":true}],"field_order":["vectara_customer_id","vectara_corpus_id","vectara_api_key","search_query","lexical_interpolation","filter","reranker","reranker_k","diversity_bias","max_results","response_lang","prompt"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"VectaraRAG-dnNiH"},"selected":false,"width":384,"height":642,"positionAbsolute":{"x":628.1577352186433,"y":303.7523159769521},"dragging":false}],"edges":[{"source":"GroqModel-3OwWV","sourceHandle":"{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-3OwWVœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-TYYi9","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-TYYi9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-TYYi9","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GroqModel","id":"GroqModel-3OwWV","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-GroqModel-3OwWV{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-3OwWVœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-TYYi9{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-TYYi9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"Memory-wsoOV","sourceHandle":"{œdataTypeœ:œMemoryœ,œidœ:œMemory-wsoOVœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-MWjss","targetHandle":"{œfieldNameœ:œchat_historyœ,œidœ:œPrompt-MWjssœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"chat_history","id":"Prompt-MWjss","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Memory","id":"Memory-wsoOV","name":"messages_text","output_types":["Message"]}},"id":"reactflow__edge-Memory-wsoOV{œdataTypeœ:œMemoryœ,œidœ:œMemory-wsoOVœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-MWjss{œfieldNameœ:œchat_historyœ,œidœ:œPrompt-MWjssœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"source":"ChatInput-MyCx4","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-MyCx4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-MWjss","targetHandle":"{œfieldNameœ:œuser_questionœ,œidœ:œPrompt-MWjssœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_question","id":"Prompt-MWjss","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-MyCx4","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-MyCx4{œdataTypeœ:œChatInputœ,œidœ:œChatInput-MyCx4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-MWjss{œfieldNameœ:œuser_questionœ,œidœ:œPrompt-MWjssœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"source":"Prompt-MWjss","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-MWjssœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"GroqModel-3OwWV","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œGroqModel-3OwWVœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"GroqModel-3OwWV","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-MWjss","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-MWjss{œdataTypeœ:œPromptœ,œidœ:œPrompt-MWjssœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GroqModel-3OwWV{œfieldNameœ:œinput_valueœ,œidœ:œGroqModel-3OwWVœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"VectaraRAG-dnNiH","sourceHandle":"{œdataTypeœ:œVectaraRAGœ,œidœ:œVectaraRAG-dnNiHœ,œnameœ:œanswerœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-MWjss","targetHandle":"{œfieldNameœ:œvector_db_resultœ,œidœ:œPrompt-MWjssœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"vector_db_result","id":"Prompt-MWjss","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"VectaraRAG","id":"VectaraRAG-dnNiH","name":"answer","output_types":["Message"]}},"id":"reactflow__edge-VectaraRAG-dnNiH{œdataTypeœ:œVectaraRAGœ,œidœ:œVectaraRAG-dnNiHœ,œnameœ:œanswerœ,œoutput_typesœ:[œMessageœ]}-Prompt-MWjss{œfieldNameœ:œvector_db_resultœ,œidœ:œPrompt-MWjssœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"source":"ChatInput-MyCx4","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-MyCx4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"VectaraRAG-dnNiH","targetHandle":"{œfieldNameœ:œsearch_queryœ,œidœ:œVectaraRAG-dnNiHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"search_query","id":"VectaraRAG-dnNiH","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-MyCx4","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-MyCx4{œdataTypeœ:œChatInputœ,œidœ:œChatInput-MyCx4œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-VectaraRAG-dnNiH{œfieldNameœ:œsearch_queryœ,œidœ:œVectaraRAG-dnNiHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"}],"viewport":{"x":-339.4380605526235,"y":368.53485060818673,"zoom":0.6661353304946205}},"description":"Design Dialogues with Langflow.","name":"Customer Service Marvit Restaurant","last_tested_version":"1.0.18","endpoint_name":null,"is_component":false}